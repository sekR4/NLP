{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d2f2c9",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "- Weights shared for each \"word cell\" (recurring)\n",
    "![image.png](img/rnn.png)\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/TyJuk/recurrent-neural-networks)\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9299d",
   "metadata": {},
   "source": [
    "# Typical RNN Tasks\n",
    "- 1:n - e.g. give a word and get a sentence\n",
    "- n:1 - e.g. give a sentence and figure out if it's offensive or not (binary classification)\n",
    "- n:n - e.g. translate a sentence into another language\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca2c38",
   "metadata": {},
   "source": [
    "# Why?\n",
    "![image.png](img/rnn_q_matrix.png)\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db1fb2",
   "metadata": {},
   "source": [
    "# Simple RNN\n",
    "![image.png](img/simple_rnn.png)\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/eaLt6/math-in-simple-rnns)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Formulas\n",
    "![image-2.png](img/rnn_math.png)\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/eaLt6/math-in-simple-rnns)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"img/rnn_element_wise.png\" align=\"left\"/>\n",
    "\n",
    "This funny circle is standing for \"*a binary operation that takes two matrices of the same dimensions and produces another matrix of the same dimension as the operands, where each element i, j is the product of elements i, j of the original two matrices. It is to be distinguished from the more common matrix product.*\" [(Hadamard product)](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) Also see: [How to do in Python](https://stackoverflow.com/questions/40034993/how-to-get-element-wise-matrix-multiplication-hadamard-product-in-numpy)\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621b593a",
   "metadata": {},
   "source": [
    "# What are we training?\n",
    "![image.png](img/rnn_what_to_train.png)\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/eaLt6/math-in-simple-rnns)\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022db7f",
   "metadata": {},
   "source": [
    "# Calculate Hidden State Activation `h` in Python\n",
    "*From h_t_prev to h_t*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9218bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d06bb5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hh = np.random.standard_normal((3,2))\n",
    "w_hx = np.random.standard_normal((3,3))\n",
    "h_t_prev = np.random.standard_normal((2,1))\n",
    "x_t = np.random.standard_normal((3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab02b180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hh\n",
      "\n",
      "[[-1.48910364  0.24731636]\n",
      " [ 2.00520266 -1.6848382 ]\n",
      " [-0.31401995  0.55496468]]\n",
      "\n",
      "w_hx\n",
      "\n",
      "[[-0.52032108  0.30678248 -0.37567689]\n",
      " [ 0.19514791  0.62584317 -1.36211149]\n",
      " [-0.91972935  0.7476726   0.66287266]]\n",
      "\n",
      "h_t_prev\n",
      "\n",
      "[[-1.80277885]\n",
      " [ 0.52402386]]\n",
      "\n",
      "x_t\n",
      "\n",
      "[[ 1.62051612]\n",
      " [-0.99302758]\n",
      " [-0.54891919]]\n"
     ]
    }
   ],
   "source": [
    "print(\"w_hh\",w_hh,\"w_hx\",w_hx,\"h_t_prev\",h_t_prev,\"x_t\",x_t, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89efe5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20711845],\n",
       "       [-0.79272032],\n",
       "       [-0.76548912]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "     return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "bias = np.random.standard_normal((x_t.shape[0],1))\n",
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408b7e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88890718]\n",
      " [0.00778221]\n",
      " [0.07548571]]\n"
     ]
    }
   ],
   "source": [
    "h_t = sigmoid(np.matmul(w_hh, h_t_prev) + np.matmul(w_hx, x_t) + bias)\n",
    "print(h_t)\n",
    "\n",
    "A = h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a25a3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88890718]\n",
      " [0.00778221]\n",
      " [0.07548571]]\n"
     ]
    }
   ],
   "source": [
    "# Another way\n",
    "h_t = sigmoid(np.matmul(np.hstack((w_hh, w_hx)), np.vstack((h_t_prev, x_t))) + bias)\n",
    "print(h_t)\n",
    "\n",
    "B = h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda99baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e6eb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88890718],\n",
       "       [0.00778221],\n",
       "       [0.07548571]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way, too\n",
    "sigmoid(np.hstack((w_hh, w_hx)) @ np.vstack((h_t_prev, x_t)) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "265212e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88890718],\n",
       "       [0.00778221],\n",
       "       [0.07548571]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way, too (this one is sexy)\n",
    "sigmoid(w_hh @ h_t_prev + w_hx @ x_t + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a7f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.concatenate([h_t_prev, x_t])\n",
    "#w_hh.shape\n",
    "#sigmoid(np.dot(w_hh, np.concatenate([h_t_prev, x_t])) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d1dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.65 µs ± 302 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sigmoid(np.matmul(w_hh, h_t_prev) + np.matmul(w_hx, x_t) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24ae8e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.8 µs ± 710 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sigmoid(np.matmul(np.hstack((w_hh, w_hx)), np.vstack((h_t_prev, x_t))) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "652f81cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.1 µs ± 1.53 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sigmoid(np.hstack((w_hh, w_hx)) @ np.vstack((h_t_prev, x_t)) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "892e10a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.59 µs ± 69.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sigmoid(w_hh @ h_t_prev + w_hx @ x_t + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63509972",
   "metadata": {},
   "source": [
    "Lets use `@` for element-wise operations. It's easier to remember, shorter and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407faff1",
   "metadata": {},
   "source": [
    "# Costs\n",
    "For one example costs can be calculated like this.\n",
    "\n",
    "<img src=\"img/rnn_costs_single.png\" align=\"left\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b558f0ef",
   "metadata": {},
   "source": [
    "If several time steps *T* are involved, we're building average costs.\n",
    "\n",
    "<img src=\"img/rnn_costs_all.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61c075",
   "metadata": {},
   "source": [
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/KBmVE/cost-function-for-rnns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe2189",
   "metadata": {},
   "source": [
    "# Scan functions\n",
    "- Abstract RNNs for fast computation\n",
    "- Needed for GPU usage & parrallel computing\n",
    "\n",
    "![image.png](img/rnn_scan_functions.png)\n",
    "\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/rhso8/implementation-note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8b811",
   "metadata": {},
   "source": [
    "An evaluation metric used in this course is perplexity, which seems not to fit with [speech recognition tasks](https://www.researchgate.net/post/What-are-the-performance-measures-in-Speech-recognition) (?).\n",
    "\n",
    "Some more intuitive explanations towards perplexity:\n",
    "- https://towardsdatascience.com/evaluation-of-language-models-through-perplexity-and-shannon-visualization-method-9148fbe10bd0\n",
    "- https://www.cs.cmu.edu/~roni/papers/eval-metrics-bntuw-9802.pdf\n",
    "- https://www.quora.com/How-does-perplexity-function-in-natural-language-processing?share=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27171fb7",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units (GRU)\n",
    "\n",
    "- In simple RNNs hidden state *h* gets updated from unit to unit\n",
    "- Problematic for long sequences (information loss each step -> vanishing gradients)\n",
    "- GRUs have additional formulas (gates) to compute to keep relevant information available through all states\n",
    "\n",
    "## Vanilla & GRU\n",
    "![simple_rnn_vs_gru.png](img/simple_rnn_vs_gru.png)\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/t5L3H/gated-recurrent-units)\n",
    "\n",
    "The symbol $\\Gamma$ looking like a 'T' without its left 'arm' is called *gamma* (Greek). It is used for the equations of 'gates' taking care of updates ($\\Gamma_u$) & relevance ($\\Gamma_r$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a33f1c",
   "metadata": {},
   "source": [
    "# Testing simple RNNs & GRUs & Scan functions\n",
    "\n",
    "Following [this](https://www.coursera.org/learn/sequence-models-in-nlp/ungradedLab/jJn3o/vanilla-rnns-grus-and-the-scan-function) lab... Although, I'm wondering why we're calculating hidden states now in a different way than we did it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f03767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "random.seed(10)                 \n",
    "\n",
    "emb = 128                       # Embedding size\n",
    "T = 256                         # Number of variables in the sequences\n",
    "h_dim = 16                      # Hidden state dimension\n",
    "h_0 = np.zeros((h_dim, 1))      # Initial hidden state\n",
    "\n",
    "w1 = random.standard_normal((h_dim, emb+h_dim))\n",
    "w2 = random.standard_normal((h_dim, emb+h_dim))\n",
    "w3 = random.standard_normal((h_dim, emb+h_dim))\n",
    "\n",
    "b1 = random.standard_normal((h_dim, 1))\n",
    "b2 = random.standard_normal((h_dim, 1))\n",
    "b3 = random.standard_normal((h_dim, 1))\n",
    "\n",
    "X = random.standard_normal((T, emb, 1))\n",
    "\n",
    "weights = [w1, w2, w3, b1, b2, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c60efcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_V_RNN(inputs, weights):\n",
    "    x, h_t = inputs\n",
    "    wh, _, _, bh, _, _ = weights\n",
    "\n",
    "    # Returning new hidden state only\n",
    "    h_t = sigmoid(np.dot(wh, np.concatenate([h_t, x])) + bh)\n",
    "    \n",
    "    return h_t, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68043c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_GRU(inputs, weights): # Forward propagation for a single GRU cell\n",
    "    x, h_t = inputs\n",
    "    wu, wr, wc, bu, br, bc = weights\n",
    "\n",
    "    # Update gate\n",
    "    u = sigmoid(np.dot(wu, np.concatenate([h_t, x])) + bu)\n",
    "    \n",
    "    # Relevance gate\n",
    "    r = sigmoid(np.dot(wr, np.concatenate([h_t, x])) + br)\n",
    "\n",
    "    \n",
    "    # Candidate hidden state \n",
    "    c = np.dot(wc, np.concatenate([r * h_t, x])) + bc\n",
    "    c = np.tanh(c)\n",
    "    \n",
    "    # Returning new hidden state only\n",
    "    h_t = u * c + (1 - u) * h_t\n",
    "    \n",
    "    return h_t, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "434a2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(fn, elems, weights, h_0=None):\n",
    "    h_t = h_0\n",
    "    ys = []\n",
    "    \n",
    "    for x in elems:\n",
    "        y, h_t = fn([x, h_t], weights)\n",
    "        ys.append(y)\n",
    "    \n",
    "    return ys, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33326beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.05 ms ± 82 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit scan(forward_V_RNN, X, weights, h_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c6289c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.81 ms ± 325 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit scan(forward_GRU, X, weights, h_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7f367",
   "metadata": {},
   "source": [
    "# Bidirectional RNNs\n",
    "\n",
    "I loved my `____`. Always when I came back from school, he was jumping around and very happy to see me.\n",
    "\n",
    "- forwarding information from both sides of the rnn\n",
    "- fill gaps in previous sentence with context from following sentence \n",
    "\n",
    "\n",
    "\n",
    "![rnn_bi_0.png](img/rnn_bi_0.png)\n",
    "![rnn_bi_1.png](img/rnn_bi_1.png)\n",
    "\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/TBXN7/deep-and-bi-directional-rnns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3240fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trax\n",
    "from trax import layers as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3f3444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 773 µs, sys: 2 µs, total: 775 µs\n",
      "Wall time: 781 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mlp = tl.Serial(\n",
    "  tl.Dense(128),\n",
    "  tl.Relu(),\n",
    "  tl.Dense(10),\n",
    "  tl.LogSoftmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68581953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Dense_128\n",
      "  Serial[\n",
      "    Relu\n",
      "  ]\n",
      "  Dense_10\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9804c9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 708 µs, sys: 0 ns, total: 708 µs\n",
      "Wall time: 714 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mode = 'train'\n",
    "vocab_size = 256\n",
    "model_dimension = 512\n",
    "n_layers = 2\n",
    "\n",
    "GRU = tl.Serial(\n",
    "      tl.ShiftRight(mode=mode), # Do remember to pass the mode parameter if you are using it for interence/test as default is train \n",
    "      tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n",
    "      #[tl.GRU(n_units=model_dimension) for _ in range(n_layers)], # You can play around n_layers if you want to stack more GRU layers together\n",
    "      tl.GRU(n_units=model_dimension),\n",
    "      tl.Dense(n_units=vocab_size),\n",
    "      tl.LogSoftmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49b59803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total layers: 5\n",
      "\n",
      "========\n",
      "Serial.sublayers_0: Serial[\n",
      "  ShiftRight(1)\n",
      "]\n",
      "\n",
      "========\n",
      "Serial.sublayers_1: Embedding_256_512\n",
      "\n",
      "========\n",
      "Serial.sublayers_2: GRU_512\n",
      "\n",
      "========\n",
      "Serial.sublayers_3: Dense_256\n",
      "\n",
      "========\n",
      "Serial.sublayers_4: LogSoftmax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_layers(model, layer_prefix=\"Serial.sublayers\"):\n",
    "    print(f\"Total layers: {len(model.sublayers)}\\n\")\n",
    "    for i in range(len(model.sublayers)):\n",
    "        print('========')\n",
    "        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n",
    "        \n",
    "show_layers(GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52fc2c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "word_ids = np.array([1, 2, 3, 4], dtype=np.int32)  # word_ids < vocab_size\n",
    "embedding_layer = tl.Embedding(vocab_size, 32)\n",
    "embedding_layer.init(trax.shapes.signature(word_ids))\n",
    "embedded = embedding_layer(word_ids)  # embedded.shape = (4, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de0d3aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-0.24339633, -0.26646966, -0.06507818,  0.13353701,\n",
       "               0.19828677,  0.2684803 ,  0.04015421,  0.1252992 ,\n",
       "               0.05146184, -0.28740436, -0.2005695 ,  0.05363747,\n",
       "               0.09167175, -0.24854366,  0.08769388,  0.281615  ,\n",
       "              -0.17424588,  0.14047112, -0.02152186,  0.02121285,\n",
       "              -0.20540592,  0.1334213 , -0.27354348, -0.05790756,\n",
       "              -0.14452775, -0.11836766,  0.17844093, -0.17286661,\n",
       "              -0.30193475,  0.21396698,  0.22410244,  0.23549293],\n",
       "             [-0.16122551,  0.10811774,  0.16578965, -0.20675819,\n",
       "               0.13521245, -0.13177456, -0.10787377, -0.06189003,\n",
       "               0.19928242,  0.10119225, -0.26748452,  0.12204586,\n",
       "               0.2817053 ,  0.21753137, -0.14413384, -0.27695698,\n",
       "               0.16268931,  0.28909573, -0.00046428,  0.26734638,\n",
       "              -0.28450522, -0.25055635,  0.06145451,  0.2477021 ,\n",
       "              -0.16908409, -0.18996347,  0.2621673 , -0.11953808,\n",
       "               0.22239962,  0.16999294,  0.04905983, -0.20106407],\n",
       "             [-0.22329988, -0.16749574, -0.12280332,  0.09156444,\n",
       "               0.23619957, -0.2903971 ,  0.18303178,  0.03630964,\n",
       "              -0.24235636, -0.25746745, -0.15453626,  0.16727462,\n",
       "               0.12095801,  0.1392893 ,  0.02715056,  0.05546256,\n",
       "               0.06114185,  0.20663795,  0.1719198 , -0.03830022,\n",
       "              -0.243126  ,  0.14999695,  0.27296284, -0.2081949 ,\n",
       "               0.12556683,  0.02291347, -0.1050404 , -0.1199106 ,\n",
       "               0.13239142,  0.06250375,  0.13422015, -0.03811005],\n",
       "             [ 0.28193787, -0.17184621,  0.1939136 , -0.04865694,\n",
       "              -0.00626833,  0.03149569,  0.26172477, -0.1165052 ,\n",
       "              -0.15344673,  0.2705771 , -0.16138662, -0.22903413,\n",
       "               0.06670273,  0.04032562, -0.13581434, -0.17382255,\n",
       "               0.2399499 ,  0.25751317,  0.28772748, -0.28509244,\n",
       "              -0.08396735,  0.22279632,  0.03472005,  0.05352614,\n",
       "               0.08109741, -0.19352977, -0.2634251 , -0.26305133,\n",
       "              -0.1387332 ,  0.30378678, -0.01942981, -0.01812872]],            dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138d378",
   "metadata": {},
   "source": [
    "# Vanishing Gradients in RNNs\n",
    "- we're using tanh & sigmoid -> gradients between -1 & 1 and 0 & 1\n",
    "- through longer sequences (or more layers), in particular through multiplication and forward/backward propagation, gradients may end up at 0 (vanish)\n",
    "\n",
    "A view strategies to deal with them\n",
    "![rnn_vanish_grad_strategies.png](img/rnn_vanish_grad_strategies.png)\n",
    "\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/OIXEN/rnns-and-vanishing-gradients)\n",
    "\n",
    "More about optimization issues: https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925c3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
