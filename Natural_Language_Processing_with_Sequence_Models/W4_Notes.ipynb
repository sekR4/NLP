{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1994d66",
   "metadata": {},
   "source": [
    "# Siamese Network\n",
    "\n",
    "can find similarities for example between new and old questions asked, which would help to answer the new question if the old one is similar.\n",
    "\n",
    "Below is an example architecture. Even though this are 2 networks, only one has to be trained, since both are using the same parameters. The only difference would be the input (e.g. different word sequences). The output vectors will be compared. The result is cosine similarity (-1 <= y_hat <= 1).\n",
    "\n",
    "![](img/siamese.png)\n",
    "\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/oUdcN/architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d90af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "import trax.fastmath.numpy as np\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6315577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(10)\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d0def9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_normalize(x):\n",
    "    return x / np.sqrt(np.sum(x * x, axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458d18a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77132064, 0.02075195, 0.63364823, 0.74880388, 0.49850701],\n",
       "       [0.22479665, 0.19806286, 0.76053071, 0.16911084, 0.08833981]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = numpy.random.random((2,5))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567cf3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.57393795, 0.01544148, 0.4714962 , 0.55718327, 0.37093794],\n",
       "             [0.26781026, 0.23596111, 0.9060541 , 0.20146926, 0.10524315]],            dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_tensor = L2_normalize(tensor)\n",
    "norm_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65c97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 500\n",
    "model_dimension = 128\n",
    "\n",
    "# A simple LSTM\n",
    "LSTM = tl.Serial(\n",
    "        tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n",
    "        tl.LSTM(model_dimension),\n",
    "        tl.Mean(axis=1),\n",
    "        tl.Fn('Normalize', lambda x: normalize(x))\n",
    "    )\n",
    "\n",
    "# Turns into a Siamese network via 'Parallel'\n",
    "Siamese = tl.Parallel(LSTM, LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1763aa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parallel_in2_out2[\n",
       "  Serial[\n",
       "    Embedding_500_128\n",
       "    LSTM_128\n",
       "    Mean\n",
       "    Normalize\n",
       "  ]\n",
       "  Serial[\n",
       "    Embedding_500_128\n",
       "    LSTM_128\n",
       "    Mean\n",
       "    Normalize\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Siamese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a5284",
   "metadata": {},
   "source": [
    "## It's all lost - How to calculate simple loss\n",
    "\n",
    "To calculate loss, we need to compare sequences. The original question is called `Anchor`, the similar one `Positive` and the completely unrelated `Negative`.\n",
    "\n",
    "\n",
    "*Do you like this course?* - Anchor\n",
    "\n",
    "*Are you happy with this course?* - Positive\n",
    "\n",
    "*Do you speak German?* - Negative\n",
    "\n",
    "![image.png](img/sim.png)\n",
    "[Source](https://www.wikiwand.com/en/Cosine_similarity)\n",
    "\n",
    "- similiarity between Anchor A and Positive P: `s(A,P) ~ 1`\n",
    "- similiarity between Anchor A and Negative N: `s(A,N) ~ -1`\n",
    "\n",
    "-> Try to minimize the difference = `s(A,N) - s(A,P)`\n",
    "\n",
    "![](img/siamese_loss_1.png)\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/Dts95/cost-function)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7a9de",
   "metadata": {},
   "source": [
    "## We're not lost. Use triplets\n",
    "Computing loss like shown above, may bring us far away from `zer0`. However, ReLU (having Loss on the y-axis and difference on x) does the trick ðŸ˜‰. We want Loss >= 0.\n",
    "![](img/triplets.png)\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/Xm3vv/triplets)\n",
    "\n",
    "Usually ReLU would go through zero. With alpha we could shift it a bit to the left/right and thereby controll loss.\n",
    "\n",
    "![image.png](img/triplet_summary.png)\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/Xm3vv/triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636cb6bd",
   "metadata": {},
   "source": [
    "## Costs, costs, costs\n",
    "\n",
    "A batch of training data may look like this:\n",
    "\n",
    "|q1 |q2  |\n",
    "--- | --- \n",
    "|How much is the fish?|What does the fish cost?|\n",
    "|How old are you?|What is your age?|\n",
    "|...|...|\n",
    "\n",
    "Within a row questions have a similar meaning. Within a column questions must have a unique meaning.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Below we see encoded question matrices.\n",
    "![](img/hard_negative_mining.png)\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/g0yAF/computing-the-cost-i)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We build a similarity matrix, which can then be used to calculate the costs.\n",
    "![](img/sim_matrix.png)\n",
    "[Source](https://www.coursera.org/learn/sequence-models-in-nlp/supplement/g0yAF/computing-the-cost-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d4a572",
   "metadata": {},
   "source": [
    "- mean negative aka mean_neg (mean of oranges in a row)\n",
    "- closest negative aka closest_neg (greatest orange in a row)\n",
    "- mean_neg accelerates training, reduces noise\n",
    "- closest_neg has higher penalties\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "This was our previous Loss function.\n",
    "\n",
    "$\\mathcal{L} = \\max{(\\mathrm{s}(A,N) -\\mathrm{s}(A,P) +\\alpha, 0)}$\n",
    "\n",
    "We replace cos(A,N) with mean_neg (cost_1) & closest_neg (cost_2).\n",
    "\n",
    "$\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
    "\n",
    "$\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
    "\n",
    "And finally we calculate the full loss.\n",
    "\n",
    "$\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21f6fe2",
   "metadata": {},
   "source": [
    "## Enough theory about costs! Python please :)\n",
    "\n",
    "### 1. Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630f5808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9946662395953002\n",
      "-0.01900636126615228\n",
      "-0.9946662395953002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculates similarity between vectors\"\"\"\n",
    "    return np.dot(v1, v2) / (np.sqrt(np.dot(v1, v1)) * np.sqrt(np.dot(v2, v2)))\n",
    "\n",
    "v1 = np.array([1, 2, 3.3, 4], dtype=float)\n",
    "v2 = np.array([1.5, 2, 3, 4], dtype=float)\n",
    "\n",
    "print(cosine_similarity(v1, v2))\n",
    "\n",
    "\n",
    "v2 = np.array([1.5, 2, 3, -4], dtype=float)\n",
    "\n",
    "print(cosine_similarity(v1, v2))\n",
    "\n",
    "v2 = -np.array([1.5, 2, 3, 4], dtype=float)\n",
    "\n",
    "print(cosine_similarity(v1, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8366f1",
   "metadata": {},
   "source": [
    "### 2. Similarity Score matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dbe23f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 9,  8,  7],\n",
       "       [-1, -4, -2],\n",
       "       [ 1, -7,  2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_1 = np.array([1, 2, 3])\n",
    "v1_2 = np.array([9, 8, 7])\n",
    "v1_3 = np.array([-1, -4, -2])\n",
    "v1_4 = np.array([1, -7, 2])\n",
    "v1 = np.vstack([v1_1, v1_2, v1_3, v1_4])\n",
    "\n",
    "print(v1.shape)\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21c147fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.14630395,  1.55176673,  3.12890109],\n",
       "       [ 6.47549455,  7.7720166 ,  7.27950611],\n",
       "       [-0.34362511, -4.94072902, -1.25822028],\n",
       "       [ 1.2771089 , -4.71240673,  3.04213296]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2_1 = v1_1 + np.random.normal(0.01, 1, 3)\n",
    "v2_2 = v1_2 + np.random.normal(0.01, 1, 3)\n",
    "v2_3 = v1_3 + np.random.normal(0.01, 1, 3)\n",
    "v2_4 = v1_4 + np.random.normal(0.01, 1, 3)\n",
    "v2 = np.vstack([v2_1, v2_2, v2_3, v2_4])\n",
    "\n",
    "print(v2.shape)\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cf2f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(v1) == len(v2), \"batch sizes must match\"\n",
    "assert v1.shape == v2.shape, \"shapes must match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057615dd",
   "metadata": {},
   "source": [
    "One could now loop over each vector combination (4x4) and calculate the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b004245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row 1\n",
      "0.9661379004611553\n",
      "0.9405050076251226\n",
      "-0.7322105968945694\n",
      "0.04546939653863544\n",
      "\n",
      "Row 2\n",
      "0.7318610222086637\n",
      "0.9874532153875015\n",
      "-0.7225398674195052\n",
      "-0.06128400251609099\n",
      "\n",
      "Row 3\n",
      "-0.7872646937342082\n",
      "-0.91262434728657\n",
      "0.9660956607450487\n",
      "0.43579320906007535\n",
      "\n",
      "Row 4\n",
      "-0.17355756401470154\n",
      "-0.3643594971478465\n",
      "0.8448608723739351\n",
      "0.9544730466989575\n"
     ]
    }
   ],
   "source": [
    "len_v = len(v1)\n",
    "\n",
    "for i in range(len_v):\n",
    "    print(\"\",f\"Row {i+1}\",  sep=\"\\n\")\n",
    "    for j in range(len_v):\n",
    "        print(cosine_similarity(v1[i], v2[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9aa237",
   "metadata": {},
   "source": [
    "However, a more elegant way is to use the dot product of L2-normalized vectors. Math ^^..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "578b80bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9661379 ,  0.94050501, -0.7322106 ,  0.0454694 ],\n",
       "       [ 0.73186102,  0.98745322, -0.72253987, -0.061284  ],\n",
       "       [-0.78726469, -0.91262435,  0.96609566,  0.43579321],\n",
       "       [-0.17355756, -0.3643595 ,  0.84486087,  0.95447305]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix = np.dot(L2_normalize(v1), L2_normalize(v2).T)\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb516e",
   "metadata": {},
   "source": [
    "Note, how nicely we can see high similarity at the diagonale ðŸ™‚."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9bca81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
